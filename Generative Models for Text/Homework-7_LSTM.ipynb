{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50oZpnr9q_TI"
   },
   "source": [
    "# USC ID: 9907399097 ; Name: Subhiksha Rani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jU1sfcPiq_TJ",
    "outputId": "e4ebbdcf-9ae0-4cbf-bb12-760634f85a43"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.(c). LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i) Full_text.txt is the concatenated file which contains all the 4 text books.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3CjWVKuq_TN"
   },
   "outputs": [],
   "source": [
    "corpus = open('Full_text.txt',encoding='utf-8').read()\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(ii) Using Character level representation by using extended ASCII__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVLDR6-Rq_TQ"
   },
   "outputs": [],
   "source": [
    "characters = sorted(list(set(corpus)))\n",
    "coded_chars = dict((c, i) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "0MUwNhnUq_TT",
    "outputId": "9f163181-7064-416d-f69f-1e18fcc2e9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1611845\n",
      "Total Vocab:  100\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(corpus)\n",
    "n_vocabulary = len(characters)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(iii) Window Size selected, W=100__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(iv) & (v)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oAuWp86Wq_TZ",
    "outputId": "1ca32f56-79cf-47e6-e55f-5444139b94f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1611745\n"
     ]
    }
   ],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - window_size, 1):\n",
    "    seq_ip = corpus[i:i + window_size]\n",
    "    seq_op = corpus[i + window_size]\n",
    "    dataX.append([coded_chars[char] for char in seq_ip])\n",
    "    dataY.append(coded_chars[seq_op])\n",
    "patterns = len(dataX)\n",
    "print (\"Total Patterns: \", patterns)\n",
    "X = numpy.reshape(dataX, (patterns, window_size, 1))\n",
    "X = X / float(n_vocabulary)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(vii) & (viii)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHL5MGiMq_Tg"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(ix) & (x)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ3CdaUeq_Tj"
   },
   "outputs": [],
   "source": [
    "weights_file=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weights_file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1091
    },
    "colab_type": "code",
    "id": "YKvlZjGMq_Tm",
    "outputId": "8435d588-0072-4f5d-bf67-16c04546fb25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1611745/1611745 [==============================] - 387s 240us/step - loss: 3.0493\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.04928, saving model to weights-improvement-01-3.0493.hdf5\n",
      "Epoch 2/15\n",
      "1611745/1611745 [==============================] - 386s 240us/step - loss: 2.8945\n",
      "\n",
      "Epoch 00002: loss improved from 3.04928 to 2.89454, saving model to weights-improvement-02-2.8945.hdf5\n",
      "Epoch 3/15\n",
      "1611745/1611745 [==============================] - 387s 240us/step - loss: 2.8276\n",
      "\n",
      "Epoch 00003: loss improved from 2.89454 to 2.82755, saving model to weights-improvement-03-2.8276.hdf5\n",
      "Epoch 4/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.7838\n",
      "\n",
      "Epoch 00004: loss improved from 2.82755 to 2.78377, saving model to weights-improvement-04-2.7838.hdf5\n",
      "Epoch 5/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.7462\n",
      "\n",
      "Epoch 00005: loss improved from 2.78377 to 2.74624, saving model to weights-improvement-05-2.7462.hdf5\n",
      "Epoch 6/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.7128\n",
      "\n",
      "Epoch 00006: loss improved from 2.74624 to 2.71278, saving model to weights-improvement-06-2.7128.hdf5\n",
      "Epoch 7/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.6830\n",
      "\n",
      "Epoch 00007: loss improved from 2.71278 to 2.68295, saving model to weights-improvement-07-2.6830.hdf5\n",
      "Epoch 8/15\n",
      "1611745/1611745 [==============================] - 389s 241us/step - loss: 2.6541\n",
      "\n",
      "Epoch 00008: loss improved from 2.68295 to 2.65409, saving model to weights-improvement-08-2.6541.hdf5\n",
      "Epoch 9/15\n",
      "1611745/1611745 [==============================] - 389s 242us/step - loss: 2.6260\n",
      "\n",
      "Epoch 00009: loss improved from 2.65409 to 2.62601, saving model to weights-improvement-09-2.6260.hdf5\n",
      "Epoch 10/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.5979\n",
      "\n",
      "Epoch 00010: loss improved from 2.62601 to 2.59792, saving model to weights-improvement-10-2.5979.hdf5\n",
      "Epoch 11/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.5702\n",
      "\n",
      "Epoch 00011: loss improved from 2.59792 to 2.57024, saving model to weights-improvement-11-2.5702.hdf5\n",
      "Epoch 12/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.5438\n",
      "\n",
      "Epoch 00012: loss improved from 2.57024 to 2.54382, saving model to weights-improvement-12-2.5438.hdf5\n",
      "Epoch 13/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.5181\n",
      "\n",
      "Epoch 00013: loss improved from 2.54382 to 2.51808, saving model to weights-improvement-13-2.5181.hdf5\n",
      "Epoch 14/15\n",
      "1611745/1611745 [==============================] - 388s 241us/step - loss: 2.4926\n",
      "\n",
      "Epoch 00014: loss improved from 2.51808 to 2.49257, saving model to weights-improvement-14-2.4926.hdf5\n",
      "Epoch 15/15\n",
      "1611745/1611745 [==============================] - 390s 242us/step - loss: 2.4691\n",
      "\n",
      "Epoch 00015: loss improved from 2.49257 to 2.46909, saving model to weights-improvement-15-2.4691.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3209b2e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=15, batch_size=4096, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The best set of weights in terms of loss is in Epoch 15 with a loss of 2.4691__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(xi)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZ_AAuGVq_Tp"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "j3pwXdBMq_Tt",
    "outputId": "0363211a-dd84-46b5-cf4e-8a171aa872dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = [coded_chars[char] for char in \"there are those who take mental phenomena naively, just as they would physical phenomena. this school of psychologists tends not to emphasize the object.\"]\n",
    "print(len(seed_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "S-nOvIGyq_Tw",
    "outputId": "f6548e42-aa74-4b76-ddf5-9b3eaf529309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is the sere th the the sore of the sere th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soene th the the sore  and the sare \"and the sored of the sere th the the soe"
     ]
    }
   ],
   "source": [
    "pattern = seed_sentence[0:100]\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocabulary)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>__Conclusion: We can see that the above predicted output isn’t accurate. We can improve the performance of the model by increasing the number of Epoch’s or by inputting more data (textbooks).__"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework-7.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
